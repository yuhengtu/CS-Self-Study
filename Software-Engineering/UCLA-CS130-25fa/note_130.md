# LEC1 Intro

exam will be multiple choice question generated by Gemini, each exam only covers half of the content, 1 hour

```
git clone ssh://yuhengtu@code.cs130.org:29418/semiconductor-savants
---
tools/env/start.sh -u yuhengtu -r -- -p 127.0.0.1:8080:8080
git checkout -b xxx
...
cd build
cmake ..
make
make test
...
cd build_coverage
cmake -DCMAKE_BUILD_TYPE=Coverage ..
make coverage
ctest --rerun-failed --output-on-failure
...
git status
git add .
git commit -m "
git review -f
---
git review -l
git review -d my_change_id
git add .
git commit --amend --no-edit
git review -f
code review +1 and submit on Gerrit web UI
---
git checkout main
git pull --rebase
git checkout my-new-feature
git rebase main
```

git rebase main, rebase is better than merge. It takes all the commits on your feature branch and replays them, one by one, on top of the latest version of main. This creates a clean, linear history



# Modern Cpp

DO NOT use raw pointers (new & delete), use Smart Pointer and RAII

![image-20251029164155208](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300741242.png)



# learngitbranching

in learngitbranching, c0/c1 is a commit hash

branch/HEAD are pointer to commits

HEAD or * is current checkout (useless: HEAD -> main (branch) -> a1b2c3 (commit), detach HEAD, HEAD -> a1b2c3)

bugFix^^ = bugFix~2 (Useless: git branch -f main HEAD~3)

git reset HEAD~1 (for local branch, not for remote branch that others use as well)

![image-20251007233704355](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510081437465.png)

git revert HEAD (share reversed changes with others)

![image-20251007233826836](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510081438894.png)

git rebase -i HEAD~3 #修改顺序

git branch -f master bugFix

![image-20251008002933616](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510081529651.png)



useless:

git cherry-pick c3 c4 c7

![image-20251008002332496](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510081523533.png)

git rebase -i HEAD~4 #修改顺序

![image-20251008002414894](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510081524931.png)

need to slightly modify an early commit c2

1. git rebase -i HEAD~2 #修改C2和C3的顺序
2. git commit --amend
3. git rebase -i HEAD~2 #修改C3'和C2''顺序
4. git branch -f master

![image-20251008003515938](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510081535982.png)

1. git checkout master
2. git cherry-pick newImage
3. git commit *--amend*
4. git cherry-pick caption

![image-20251008004654160](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510081546256.png)

git tag v0 c1: branch is fragile, tag is permanent, you can't check out a tag and commit on the tag

git describe <ref>, <tag>-<numCommits>-g<hash>: describe where you are relative to the closest tag

git checkout main, goes to c1; git checkout main^2, goes to c2

![image-20251012225601943](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510131356056.png)

git branch bugWork HEAD\~\^2~

![image-20251012225815624](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510131358666.png)



# LEC2 Test

Unit Test/Integration Test/End-to-End Test

testable code:

- Testing boundaries: do not test main loop

- Refactoring for testability: e.g., extract a handler
- make the fn return sth instead of void
- Dependency Injection (e.g., can pass in the test db or real db to the class construction), Inversion of Control (IoC)
  - composition (has-a) > inheritance (is-a)
  - new is harmful, get dependency from outside (new clock in the main fn)

test object:

- real object
- fake object (simulate real)
- mock object (input -> output)

fake > mock

test what is the output (1,2,3) instead of how is the output (internal behavior)

Test readability:

- DRY: Don't Repeat Yourself
- DAMP: Descriptive and Meaningful Phrases



Unit tests is for single componenet

Integration test tests the componenet connection

- Slow -> focus on happy path tests
- hard to write / debug

Test script of the webserver running binary:

- Start the server process

- Send it an HTTP request
- Capture the server's response
- Verify the response is correct
- Shut down the server process

![image-20251015191619168](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510161016210.png)

Make your integration test part of main build (can be run with `make test`)

```
add_test(NAME web_server_integration_test
         COMMAND ${CMAKE_CURRENT_SOURCE_DIR}/tests/my_test.sh
         WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/tests)
```



# HW1 Nginx Config

- directives: 

  - Simple directives: worker_processes 1;

  - Block directives: 

    http {
        include       mime.types;
        default_type  application/octet-stream;
    }

- context: block directives that can contain other directives

  http {
      server {
          listen 80;
      }
  }

- main context: directive placed outside of any block



# LEC3 HTTP & Commit message

HTTP request (client to server)

- There is \r\n (CRLF 回车换行) at the end of each line

- The request ends with a blank line (\r\n\r\n)

- e.g.:

  GET /index.html HTTP/1.1

  Host: www.example.com

  User-Agent: curl/7.68.0

  Accept: * / *

  *<blank line>*

1. The Request Line

   GET /path/to/file.html HTTP/1.1

   GETorPOST /path/to/file.html(URI) HTTP/1.1(HTTP protocol version)

2. Zero or more Request Header Lines

   Host: www.example.com (Required in HTTP/1.1)

   User-Agent: Mozilla/5.0 (X11; Linux x86_64) ...

3. A single Blank Line (\r\n)

4. An optional Message Body (Not used for GET requests)

   - POST (create a new entry)
   - PUT (update an existing entry)



HTTP response (server to client)

- The body of the e.g. response is the complete request from our earlier example

- e.g.:

  HTTP/1.1 200 OK

  Content-Type: text/plain

  Content-Length: 88

  *<blank line>*

  GET /index.html HTTP/1.1

  Host: www.example.com

  User-Agent: curl/7.68.0

  Accept: * / *

1. The Status Line

   HTTP/1.1 200 OK

   Status Code (200 OK, 404 Not Found, 400 Bad Request, 500 Internal Server Error) + Reason Phrase (A short, human-readable text description of the status code)

2. Zero or more Response Header Lines

   - Content-Type: text/plain

     MIME type of message body, e.g., text/html, image/jpeg, application/json

   - Content-Length: 123

     Size of message body in bytes

3. A single Blank Line (\r\n)

4. An optional Message Body



curl -v http://localhost:8080/echo

- -v (verbose) flag is essential

- It shows you the raw HTTP request curl sends
- It also shows you the raw HTTP response your server sends back

browser developer tools

- F12
- Ctrl+Shift+I
- Cmd+Opt+I on Mac



The #1 rule: make your changes small

- Refactoring First and Only
- Backend first then frontend
- Submit new class definition and header file first for design feedback, add method implementations in subsequent changes.

change description

```
#What Updates authentication to support passwordless @google.com login.

#Why According to #ticket-3 >70% of our users have a google.com email and surveys from PR #ticket-8 show requests for smoother password-less logon using accounts such as Facebook or Google.

#How When a user attempts to sign in they'll have an option to use an email/password or click a google-login button that steps through the oauth flow in <design doc link>, using redirect /oauth_login/google/user?access_token.
Screenshots of the login screen in design doc <link>.

#Testing
 - Added a test account test-user-login@google.com
 - Added selenium tests that exercise the login
```



# LEC4 Cmake & Docker

CMake: automated, efficient, portable building of C++ projects (dependencies & compiler commands)

- Reads configuration files named CMakeLists.txt
- Detects OS, compiler, and installed libraries
- Generates native build files

1. cmake .. 

   Run from a build directory to generate Makefiles

2. make

   Use the generated Makefiles to build your code

Continuous Integration: automatically build and test your code every time you submit a change.



Docker: package the app and its environment together

Image: Snapshot of OS defined by Dockerfile, created by `docker build`, starts from a base image (e.g. ubuntu), can be tagged with versions (e.g. web:stable)

Container: an instance of Image, created by `docker run`. When you stop a container, you lost everything.

Volume/Bound mount (directory on host VM): permanently store data from the container or share data among containers

Port: Expose container ports to: Host (use 127.0.0.1), Internet (use 0.0.0.0). Define in: Image (Dockerfile) for documentation, Container (docker run) for implementation

![image-20251009164215986](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510100742114.png)

Write a Dockerfile, each instruction creates a layer in the image, syntax: INSTRUCTION arguments

1. Define **base image**: 

   - FROM ubuntu:22.04

   - FROM postgres:14-alpine

   - FROM gcr.io/my-project/my-image:latest

2. **RUN** <command>: Install packages, compile code, create directories

   - RUN apt-get update &&

       apt-get install -y \

       build-essential \

       cmake \

       libboost-all-dev

3. **COPY** <src> <dest>: Copy files from local machine into the image

   - COPY . /usr/src/app (copy files in . to /usr/src/app)

   - COPY build/bin/server /usr/bin/

4. **WORKDIR** /path/to/work/dir: change directories, same as cd

5. **EXPOSE** <port>: Declare a port within the container should be exposed externally, just for documentation, requires -p with docker run to actually publish the port

6. **ENTRYPOINT** ["executable", "param1"]: Specifies the binary to run within the container

   **CMD** ["param2", "param3"]: Provides default arguments to ENTRYPOINT

   - ENTRYPOINT ["/usr/local/bin/webserver"]
     CMD ["--config", "/etc/server.conf"]

7. **Multi-stage builds**: build (compile), then deploy (run)

   2 FROM instruction, each begins a new build stage

   - **Stage 1:** **builder**

     - A large image with all our build tools (g++, cmake, etc.).

     - Used to compile our C++ code.

   - **Stage 2:** **deploy**

     - A minimal, clean base image (like ubuntu:latest).

     - copy only the compiled binary from the builder stage.

   Benefits: Small final image size

```dockerfile
# Stage 1: The Builder
FROM cs130/devel as builder

# Copy all source code into the image
WORKDIR /usr/src/project
COPY . .

# Create a build directory and compile the code
WORKDIR /usr/src/project/build
RUN cmake ..
RUN make

# Stage 2: The Final Deployment Image
FROM ubuntu:latest

# Copy the compiled binary from the 'builder' stage
COPY --from=builder /usr/src/project/build/bin/webserver /usr/local/bin/

# Copy the server configuration file
COPY config/server.conf /etc/server.conf

EXPOSE 80
ENTRYPOINT ["/usr/local/bin/webserver"]
CMD ["/etc/server.conf"]
```

docker build -t <image_name>:<tag> .

- . is the "build context"
- docker build -t my-webserver:v1 .

docker run -p <host_port>:<container_port> <image_name>:<tag>

- docker run -p 8080:80 my-webserver:v1
- map host port 8080 to container port 80

docker run -d ...

- run the container in detached mode (in the background)
- docker run -d -p 8080:80 --name my-server my-webserver:v1

docker ps

- Lists all currently running containers

docker stop <container_id_or_name>

- Stops a running container gracefully



# LEC5 Cloud Deployment & Continuous Integration & Release

1. Package your application into a self-contained Docker image

   gcr.io/<project-id>/<image-name>:<tag>

   gcr.io/cs130-223806/nginx:latest

   Use `docker tag` to apply full name to local image

2. Push that image to a container registry

   container registry: central, cloud-based storage for docker image (docker push, docker pull).

   E.g., Docker Hub, Google Artifact Registry

3. Build in the cloud (Google cloud build)

   cloudbuild.yaml

   ![image-20251015183943382](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510160939529.png)

   `gcloud builds submit --config docker/cloudbuild.yaml .`

   “.” specifies build context

4. Run the image from a cloud server (VM) that pulls from the registry (Google compute engine)

   Firewall Rule, Check the "Allow HTTP traffic" box, This opens port 80 to the public internet

   `gcloud compute ssh your-vm-instance-name`

   debug

   - docker ps, docker logs
   - curl to localhost at VM



Continuous Integration (CI)

- Frequently merging all developers' code into the main branch

- Each merge automatically triggers a build, Each build automatically runs a suite of tests

- Goal: Detect integration errors as quickly as possible.

Green/Red build

Set up CI (Cloud Build doesn’t integrate with private Gerrit):

- Create a read-only mirror of Gerrit repository inside Google Cloud
- Gerrit Repo → Submit → Cloud Source Repo → Trigger → Cloud Build

Creating a Cloud Build trigger

- Event: Push to a branch

- Source: Your mirror repository

- Branch: ^main$ (regular expression (regex) used to exact match the main branch)

- Configuration: The path to your cloudbuild.yaml file

Watch the cloud build Histroy. cloudbuild.yaml does build -> test -> push, any step fails, push does not happen



`:latest` tag points to the most recent version, automatically changes after each successful build.

-> DO NOT use `:latest` in production (VM run)

QA cycle stands for a Quality Assurance cycle

For Deliberate Release, `:release` usually means stable version, `:dev` or `:testing` means edge version

![image-20251015190434789](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510161004838.png)

Suggested release workflow:

- CI automatically builds and tests every commit, creating new `:latest`

- When the team decides to release, the TL pulls the current `:latest` image
  - `docker pull my-image:latest`

- The TL performs a final manual check (e.g., runs it locally)

- If it’s good, they re-tag the image and push:

  - `docker tag my-image:latest my-image:release`

  - `docker push my-image:release`

- Restart the production VM to pick up the new `:release` version



# LEC6 Webserver architecture & Logging

Webserver architecture

- Request handlers

- Request container & parser

- Response container & generator

- Dispatcher (router)
- ...



use logging libraries (Boost.log)

**log rotation** (to deal with large log files, logging libraries can handle automatically), e.g.,

- Cap usage at 100MB
- When current log file grows larger than 10MB:
  - Rename log file and timestamp it
  - Start a new log file
- If more than 10 log files, delete the oldest one

saves log in remote (cloud), not in container



# LEC7 Static Analysis & Code Style

static analysis: find bug in code without executing it (can be false positive/negative)

- memory errors:

  - use after free (access memory after delete)
  - buffer overrun (write past end of array)

- logic concurrency errors:

  - deadlock (Thread A locks Mutex 1 then waits for Mutex 2, Thread B locks Mutex 2 then waits for Mutex 1)

    Mutex (mutual exclusion): Thread A: lock the mutex → access the data → unlock, Thread B: wait until mutex unlock

  - dead code (condition impossible to be True)

-> add metadata to code as hint to static analyzer

![image-20251026122614402](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270326536.png)

![image-20251026123343046](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270333097.png)

static analyses:

- clang-tidy:

  - Powerful linter that finds common programming errors, style violations, and bugs

  - Configurable super-set of compiler warnings

- clang-format:

  - Automated code formatter

  - Reformats your code to match a consistent style guide

runtime analysis: runs program in a special, instrumented env that watches for bad behavior

- memcheck:
  - Watch for memory leaks, invalid reads/writes (buffer overflows), use of uninitialized variables
  - Very slow

coverage report:

- gcovr:
  - prove which line of the code is not tested



Code Style

![image-20251026124053714](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270340764.png)

function should do one thing

comment should only help reader save time

![image-20251026124219087](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270342129.png)

don't use tmp, data, a as variable name

![image-20251026124909904](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270349929.png)

Google C++ Style:

- **k** prefix for constants
- **CamelCase** for class names
- **lower_snake_case** for variable names
- Trailing underscore for private member variables

![image-20251026125209668](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270352698.png)

Typed units

![image-20251026124823262](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270348306.png)

Break code into logic paragraphs

avoid nesting

![image-20251026125443626](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270354679.png)

avoid control flow variable

![image-20251026125531098](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270355146.png)

make variable less visable when possible

![image-20251026125631773](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270356816.png)



# LEC8 Refactor & Design Pattern & Debug

Refactor -> pass test

parameter object

![image-20251026132827623](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270428725.png)

factory method instead of constructor

call `auto server = HttpServer::Make(80);` instead of normal constructor `HttpServer server(80);`

![image-20251026141037874](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270510980.png)

builder

![image-20251026141300678](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270513735.png)



Design Patterns: general, reusable solutions to common problems in software engineering

1. Observer: e.g., when open a file, multiple things happen: update UI, record log, ...

   Instead of writing them one by one, we can throw them to observer

   ![image-20251026171151906](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270811999.png)

2. Lazy initialization: for expensive initialization, create when requested

   ![image-20251026171516848](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270815887.png)

3. Factories

   ![image-20251026172413474](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270824515.png)

4. Singleton: a class only have one global instance

   ![image-20251026172659687](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270826729.png)

5. Pools/freelist: resources that are expensive to create when requested (e.g., database connection), pre-allocate a pool of them; take from pool and return to pool for reuse

   ![image-20251026173013682](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270830728.png)

6. RAII (Resource Acquisition Is Initialization): Tie the lifetime of the resource to the lifetime of an object on the stack

   - `std::unique_ptr` and `std::shared_ptr` for memory
   - `std::lock_guard` for mutexes
   - `std::ifstream` for files

   ![image-20251026173453951](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270834014.png)

7. Decoraters: add features at run time without changing code behavior

   ![image-20251026173716008](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270837059.png)

8. Continuation: avoid long running task to block main thread (async in Python)

   ![image-20251026174511738](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270845812.png)

9. Strategy: dynamically execute streategy at run time

   ![image-20251026174957314](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510270849363.png)



Debug: reproduce -> hypothesis -> experiment -> fix bug and write a new test

GDB/LLDB:

- Set Breakpoints
  - break server.cc:123
  - break MyClass::MyMethod
  - Conditional: break server.cc:123 if user_id == "guest"

- Control Execution:
  - run: Start the program.
  - continue: Run until the next breakpoint.
  - next: Execute the next line (step over functions).
  - step: Execute the next line (step into functions).

- Inspect State:
  - print my_variable
  - backtrace: Show the current call stack.
- change variables on the fly

Binary test search: After 100 commits, sth fail -> use `git bisect`, an automated binary search through your commit history

- Takes a "good" commit (test passes) and a "bad" commit (test fails)
- Git checks out a commit in the middle
- You run the test
- Tell it if it's "good" or "bad"
- Git cuts the search space in half and repeats (binary search)



# LEC9 API Design

API Design: simple, hard to misuse; do one thing; generalize across repeated code, extensible; hide implementation details; do not take concrete classs (e.g., a real database); once released cannot delete, be careful to add argument

![image-20251029163337927](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300733977.png)

![image-20251029164030324](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300740422.png)

![image-20251029164050707](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300740759.png)

![image-20251029164618800](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300746869.png)

design example of CSVReader

1. Builder

![image-20251029165424075](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300754142.png)

![image-20251029165433754](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300754804.png)

2. read line by line to save memory

![image-20251029165552575](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300755647.png)

![image-20251029165603950](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300756009.png)

3. use line name instead of index to locate

![image-20251029165729361](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300757425.png)

4. error handling

Change the return type of Read() to a type that can hold either a successful result or an error status

- Common pattern in modern C++ and other languages
  - e.g. Go’s *(result, err)* or Rust’s *Result<T, E>*

Use StatusOr<T> object:

- Contains a status
  - e.g., *OK, NOT_FOUND, PARSE_ERROR*
- If status *OK*, it also contains the resulting value *(T)*

Client checks status before accessing value

![image-20251029165933719](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300759782.png)

Finally,

![image-20251029170042451](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202510300800516.png)



# LEC11 Safety

from user story to technical task

Kanhan board: Trello, GitHub project, physical whiteboard



Path Traversal: Read files outside the web root, like /etc/passwd

- Server configured to serve files from a specific directory, the web root, e.g. /var/www/html/static
- Server should be “jailed” inside this directory, unable to access files above it.

Consider `GET /static/../../../../../etc/passwd`: `full_path = "/var/www/html/static/../../../../../etc/passwd"`

Defence:

1. URL Decode the request path
2. Check if the path contains any `..` sequences, reject if found (400 Bad Request)
3. Construct the full path (`web_root + request_path`) and use a library function to resolve it to its absolute, “canonical” form
4. Check if the resulting canonical path starts with the canonical path of your web root, reject if not



Consider `POST /api/Shoes` with malformed JSON

- Invalid Input: `{ "brand": "Nike", "size": 10`
- Empty Body: the POST request has no body at all
- Wrong Data Types: the user sends a string where you expect a number: `{ "brand": "Nike", "size": "large" }`
- Missing Fields: a required field is missing: `{ "brand": "Nike" }`
- Extra Fields: the user sends fields you don’t recognize: `{ "brand": "Nike", "size": 10, "isAdmin": true }`

Your code must validate all of these cases and return 404 if invalid:

- Is the request body present if required?
- Can the body be parsed correctly (e.g., as JSON)?
- Are all required fields present in the parsed data?
- Do all fields have the correct data type (string, number, boolean)?
- Are the values within an acceptable range (e.g., `size` must be > 0)?



400 status code tells the client that their request was bad. A good API also tells them why it was bad

<img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121639872.png" alt="image-20251112163953718" style="zoom:50%;" />

Client errors (4xx): client's request is flawed

- 400 Bad Request
  - Use for syntax errors, e.g., The request body is not valid JSON.

- 404 Not Found
  - Use when the specific resource doesn't exist, e.g., `GET /api/Shoes/999` and shoe #999 doesn't exist.
- 405 Method Not Allowed
  - Use when the client uses the wrong HTTP method for a valid URL, e.g., `POST /api/Shoes/123`. The URL is valid, but you can't POST to a specific entity's URL.

Server errors (5xx): The request was valid, but the server failed to fulfill it, need to debug!

- 500 Internal Server Error
  - Indicates a bug or an unexpected failure in your system, e.g., An unhandled exception is thrown; Your server can’t write to the filesystem (e.g., disk full, permissions error); A logic error leads to an impossible state.

![image-20251112165121678](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121651747.png)

![image-20251112165145755](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121651812.png)



Access Control

- Authentication (AuthN): “Are you who you say you are?”
  - This is what a login page does (username/password, Google Sign-In, etc.).
  - The output is a verified identity (e.g., “This is user joebruin@ucla.edu”).

- Authorization (AuthZ): “Now that I know who you are, are you allowed to do this?”

The principle of least privilege: A subject should be given only those privileges needed for it to complete its task.

- Start with zero permissions.
- Explicitly grant only the specific permissions that are absolutely necessary. Default to "deny."

Access Control List (ACL): specifies which users (or groups) are granted which permissions on a particular object.

- Object: The thing being protected (e.g., a file, a “Shoe” entity).
- Subject: The user or process trying to access the object.
- Permission: The action the subject is trying to perform (e.g., READ, WRITE, DELETE).

![image-20251112170855750](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121708890.png)



Denial of Service (DoS)

- An attacker connects and sends a Content-Length: 1073741824 header (1 Gigabyte).
- If the server tries to read the entire request body into memory before parsing, it will allocate a 1GB buffer.
- A few of these requests are enough to exhaust all the RAM on your small cloud VM.
- The operating system will start swapping to disk, performance will plummet, and eventually the server process will be killed by the OS for using too much memory.
- Your service is down.

Defence:

- Check the Content-Length Header: If too large, do not read the body.
- Reject Immediately: Send back 413 Payload Too Large status code and close the connection.

Defense in Depth:

- What if the client lies about the Content-Length?
- As you read the body, count the bytes. If the client lied, stop reading, send 413, and close the connection.



Log personally identifiable information (PII)

- Only log PII when absolutely necessary with a plan to protect and eventually delete it!

What counts as PII?

- Direct Identifiers

  - Full Name

  - Email Address

  - Social Security Number

  - Home Address

- Indirect Identifiers (Linkable PII)

  - IP Address

  - Device ID

  - Geolocation (Latitude/Longitude)

  - Date of Birth 

PII Laws: GDPR (General Data Protection Regulation) in Europe and CCPA (California Consumer Privacy Act)

- Consent: You can’t just collect data. You have to ask first, and the user must be able to opt-out.

- Access / Data Portability: be able to provide a user with a complete copy of all the data you have stored about them.

  Engineering Challenge: How to find all of a user’s data when it’s spread across dozens of different databases and log files?

- The Right to be Forgotten: When a user deletes their account, you must delete all of their PII from your systems.

  Engineering Challenge: This is extremely difficult in complex, distributed systems with backups and append-only logs.

Many big data & logging systems are append-only

- Many large-scale data systems (Kafka, Hadoop HDFS logs, BigQuery logs, ClickHouse tables, etc.) are designed to be append-only, meaning:
  - You only add new records to the end
  - You do not modify or delete old records (at least not immediately)

Engineering solutions:

- Solution 1: Aggregation: Instead of storing raw, event-level logs forever, process them and store only the summary statistics.

  - can get total visitors, but cannot get total number of visitors

- Solution 2: Pseudonymization: Instead of storing the real identifier, store a non-reversible, pseudonymous one.

  - Request comes in for `visitor_id = "user123"`.

  - Don’t log the ID directly, compute a cryptographic hash of it:

    - `hashed_id = HMAC-SHA256("user123", secret_key)`

    - This produces a garbage-looking string like `a1b2c3d4…`

  - Log the `hashed_id`, not the real ID.

Real anonymization is hard!

- Data can be "re-identified"
  - For example, if you log a hashed user ID along with their exact latitude and longitude, you can probably figure out who they are.
- The space of identifiers matters
  - Hashing an IP address is not very secure because there are only 4 billion possible IPv4 addresses. An attacker could pre-compute all of them.



Newer processes

- Privacy Design Documents:

  - Similar to a technical design doc, but focused specifically on data handling

  - What PII are you collecting?

  - Why are you collecting it? (Data minimization)

  - How long will you store it? (Retention policies)

  - How will you delete it?

- Privacy Review Councils:

  - Dedicated team of privacy experts who review and approve designs

- Privacy Approvals for Launch:

  - Cannot launch a new feature without a formal sign-off from the privacy team



# LEC12 Scale-up Problem

Your single server is a success, getting more requests and hitting resource limits

- Option 1: Big Iron / Vertical scaling / The “mainframe” or “supercomputer” approach:

  - Make one server bigger and more powerful, more CPUs, more RAM, faster disks

  - Historically, these machines were called “big iron”
    - Built with redundant, hot-swappable hardware
    - Designed for extreme reliability

  - Cons:
    - Extremely expensive, with diminishing returns
    - Doubling the power costs much more than double the price
    - There is a physical limit to how big and powerful a single server can become (hard to power or cool)
    - Still a Single Point of Failure: A datacenter issue (power, network) will still take you offline

- Option 2: Distributed system / Horizontal scaling / a “cluster” or “fleet”
  - No single point of failure



User to datacenter for Internet

<img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121749024.png" alt="image-20251112174936887" style="zoom:25%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121749222.png" alt="image-20251112174950160" style="zoom:25%;" />

e.g.

- Load balancer (manager) and web servers (workers)
- MapReduce master (manager) and task nodes (workers)
- Bigtable master (manager) and tablet servers (workers)



Failure mode at scale

- Correctness bugs: 

  - Code bug, catch from tests

- Performance bugs at scale:

  - Code is correct, hard to catch in small-scale tests

  - Often emerge from unexpected interactions between components

Performance bugs failure modes

1. Producer-consumer rate mismatch

- A producer creates tasks and adds them to a queue, e.g., A web frontend receiving user uploads
- A consumer pulls tasks from the queue and executes them, e.g., A backend worker processing those uploads
- rate(producer) > rate(consumer) so queue grows indefinitely
- e.g., new year eve people uploading too many photos, old uploads take hours of waiting time, new uploads might fail entirely -> system crush, single server still working

Address:

- Backpressure: If the queue gets too long, it should signal the producers to slow down or stop accepting new work (e.g., temporarily disable uploads)
- Autoscaling Consumers: The system should automatically add more worker machines to the consumer fleet when the queue length grows, and remove them when it shrinks

2. hotspots / unbalanced node (twitter the fail whale from celebrity tweets):

- Systems often distribute load by user or product ID
- Some users / data way more popular than others
- Shards responsible for "hot" data gets overwhelmed. The rest of the system might be idle.

Address:

- Caching: Serve hot data from fast, in-memory cache instead of the database. A viral tweet can be cached once and served to millions of users without touching the database
- Architectural Changes: Twitter eventually re-architected its timeline delivery, moving from a "pull" model to a "push" model for popular accounts (pull: when user refresh, twitter do a new query; push: when a popular tweet is posted, it is pre-pushed to all followers)

![image-20251112185939200](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121859298.png)

3. Synchronous global config rollout (2019 google cloud outage)

- Servers designed to fetch and apply latest config at the same time to ensure consistency
- Single bad change (e.g., a typo, a bad regex, an incorrect timeout value) is pushed to the config repo
- Every server pulls the bad config and starts failing in the exact same way at the exact same time

Address

- Staged Rollouts: Never deploy a change to 100% of your fleet at once

  - Start with a single server ("canary")

  - Expand to a single cluster, then a single region

  - Monitor for problems at each stage before proceeding

- Automated Rollbacks: Your deployment system should automatically halt and reverse a rollout if it detects a spike in errors.

![image-20251112190629024](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121906124.png)

4. Amplified rare events (2019 google cloudfare outage)

- one-in-a-billion events happen constantly with 100 million requests per day

Address:

- Performance test everything

  - In a large-scale system, no such thing as a "trivial" or "harmless" change

  - A single line of code (or regex!) can be a single point of failure with global impact

  - The "rare event" is often a specific user input that code is not prepared for

- Expert Code Review: Performance-sensitive components like regular expressions or parsing libraries require expert review

- Fuzzing: Use automated tools to send a wide variety of random and malicious inputs to find these pathological edge cases before users do

5. 2008 pakistan knock down youtube

- In 2008, the government of Pakistan ordered its ISPs to block access to YouTube nationwide
- Pakistan Telecom then attempted to do this by creating a "black hole" route for YouTube's IP addresses within their own network. They accidentally broadcast this malicious route to the entire internet 

![image-20251112192013049](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511121920169.png)

Address:

- Design for resilience against external failures! Services depend on a huge number of external systems (DNS, BGP, other networks)

- Monitoring: YouTube's engineers detected the problem because their monitoring systems saw that global traffic had dropped to zero.
- Mitigation: They were able to fix it by announcing even more specific routes to reclaim their traffic, and by working with other network operators to block the malicious announcements.



Performance planning: Performance optimization is endless, need to define an exit criteria:

- When users can scroll at a smooth 60fps
- When the page loads “instantly” (~100ms)
- When it works well on crappy hardware and slow networks

Google's web performance model: RAIL (Response, Animation, Idle, Load)

- Response
  -  Goal: Acknowledge user input in under 100 milliseconds (so that human do not feel delay)
  - The key is to provide feedback if above 100 milliseconds:
    - Show a loading spinner
    - Disable the button to prevent double-clicks

- Animation
  - Goal: Produce each frame of an animation in under 16 milliseconds (To achieve a smooth, fluid visual experience (like scrolling), you need 60 fps, 1 second / 60 frames = 16.67ms per frame). If any frame takes longer than 16ms to compute and draw, the user will see a stutter or “jank.”
- Idle
  - Goal: When the user isn't doing anything, the application should also be doing as little as possible
  - The main thread should be free so it can respond to the next user interaction instantly
  - Break up long-running background tasks into smaller chunks of less than 50ms
  - Even if a user clicks right in the middle of a background task, the app can still respond within the 100ms budget
- Load
  - Goal: Load the page and become interactive in under 1 second (Not everything has to be fully loaded! Just the main content so the user can start interacting with it)
  - Progressive loading: Load the critical, visible content first, and defer everything else (like images below the fold, or less important scripts) until later

We can't always improve actual performance, but we can almost always improve perceived performance.

- Actual performance: How long it takes for a task to complete

- Perceived performance is what truly matters: How long it feels like the task took

  - Loading Shims / Skeletons: Instantly show a grayed-out “skeleton” of the UI layout

  - Progress Bars: Provide a sense of forward momentum for long-running tasks.

    Even a fake or non-linear progress bar is better than nothing!

  - Optimistic UI Updates: Update the UI immediately, assuming an operation will succeed

    Example: When you “like” a post, the button turns blue instantly, even before the server has confirmed the action.



Capacity Planning

At extreme loads, systems often stop scaling linearly (not Capacity = N * Capacity of one server)

- Adding more servers might yield diminishing returns, or even make things worse!
- Shared resources become bottlenecks: e.g., the database, a central cache, the load balancer itself
- Network congestion increases
- Increased coordination overhead between servers

![image-20251113151812112](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131518308.png)

Modern cloud platforms offer autoscaling, which can help, but needs data

- Automatically adds or removes servers based on metrics like CPU utilization
- Autoscaling systems often assume a linear response
  - "If average CPU is over 70%, add another server."
- If your system behaves non-linearly, simple autoscaling rules can fail
- To configure autoscaling correctly, you need real data about how your system behaves under stress

The solution: Load testing, Intentionally overwhelming your system with simulated traffic in a controlled environment

- The goal is to generate the performance curve

- This is the only way to gather the data needed for accurate capacity planning

- Load testing must be done proactively before users trigger it

- This allows you to

  - Set accurate autoscaling policies

  - Predict how many servers you'll need for next year's traffic
  - Identify hidden bottlenecks in your system, e.g. database query that only appears at high load

![image-20251113152547445](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131525520.png)



# LEC13 Site Reliability Engineering

Site Reliability Engineering (SRE): Use software engineering principles to make services more reliable and scalable

Monitoring is the foundation

![image-20251113152925083](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131529120.png)

Production terminology

- Job: A single process running on a machine.

- Service: A collection of identical jobs, usually sitting behind a load balancer.

![image-20251113153049949](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131530989.png)

Black-box monitoring

- OS-level monitoring

- Agnostic of application state
- No integration needed

- Examples: CPU usage, Disk space, Network bandwidth, Process is running

White-box monitoring

- Application-level monitoring
- Exposes internal state
- Requires instrumentation
- Examples: Number of requests, Response codes, Exceptions, Latency to other services



Instrumentation: we need application to report its white-box metrics

Three common methods:

- The Pull Model: Expose a metrics endpoint (e.g., /metrics) that an external service read from
- The Push Model: Write structured logs to stdout, push them to a central service
- Human-Readable: Expose a human-friendly status page for live debugging

1. The Pull Model

Need a central, thread-safe place (need mutex, because multiple threads may update them at the same time) to store metrics:

![image-20251113154051914](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131540952.png)

create a monitor handler

![image-20251113154138892](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131541958.png)

Add calls to Increment wherever an interesting event occurs in your code:

![image-20251113154236628](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131542699.png)

` GET /monitoring` to see. The format (Prometheus exposition format) is simple, human-readable, and machine-parsable

![image-20251113154410754](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131544828.png)

Monitoring services scrape the data to create a time-series: Every N seconds, monitoring requests `/metrics` endpoint

![image-20251113154651020](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131546091.png)

2. The Push Model

Structured logs

- Machine parseable; Not human-readable sentences
  - Cloud platforms (like Google Cloud Logging) are designed to automatically parse structured logs into metrics
- Common formats include key-value pairs or JSON
- Use regular expressions to extract
  - e.g., the response code, `... code=200 ...`, Regular Expression: `code=([0-9]+)`
  - e.g., handler name, `... handler="HealthHandler" ...`, Regular Expression: `handler="([^"]+)"`

![image-20251113154916416](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131549496.png)

![image-20251113154935559](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131549631.png)

![image-20251113155344229](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131553310.png)

3. Human-Readable: live debugging

- Uptime: How long has this process been running?
- Configuration: What config file and flags was it started with?
- Request Counts: Total requests handled, broken down by handler or response code.
- Cache Statistics: Hit/miss ratio, number of items in the cache.
- Recent Errors: A log of the last N error messages.
- Build Information: What version of the code is this job running? (Git hash, build timestamp).



Use monitoring data for alert and dashboard

![image-20251113162254608](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131622730.png)

PromQL or SQL-like variants

- To get requests per second: 
  - PromQL: `rate(requests_total[5m])`
  - Calculates the per-second average change rate of the `requests_total` counter over the last 5 minutes



A good dashboard

- easily understandable at a glance, i.e., within 5–10 seconds
- Focuses on high-level service health, not implementation details
- Provides links to more detailed information
- Is tailored to its audience, e.g., SREs, developers, executives

<img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131734924.png" alt="image-20251113173425886" style="zoom:25%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131734496.png" alt="image-20251113173450459" style="zoom:25%;" />

4 Golden Signals

- Latency: The time it takes to service a request
  - Distinguish between the latency of successful and failed requests, Failed requests might be artificially fast!
  - Don't just track the average, which hide outliers. Track the distribution: 50th, 90th, 95th, and 99th percentiles

- Traffic: A measure of how much demand is being placed on your system
  - For a web server, this is typically measured in requests per second
  - Helps contextualize the other signals. Is a rise in errors due to a bug, or just a massive spike in traffic?

- Errors: The rate of requests that fail

  - Failures can be explicit (HTTP 500) or implicit (HTTP 200 OK but with the wrong content)

  - A rising error rate is one of the most direct and urgent indicators of a problem

    This calculates the fraction of all requests that are resulting in a 5xx error:

    ![image-20251113173335404](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131733535.png)

- Saturation: How “full” your service is; how close it is to its capacity limit
  - For constrained resources, e.g. memory, CPU, disk space
  - Examples: CPU utilization at 90%, Free space on a persistent disk at 10%, Length of a request queue over a threshold



Alert: so that you don't need to stare at dashboard

Depending on how urgent the alert is, we can (don't alert too much, otherwise people ignore it):

- File a bug, Low priority, non-urgent
- Send an email, Informational, can wait until tomorrow
- Page during work hours, Urgent, but not an emergency
- Page 24x7, Critical emergency, user-facing outage

A good alert is actionable, guides the on-call engineer with:

- What is broken (the trigger or user impact)
- Where to look first (link to a dashboard)
- What to do (link to a playbook or runbook), A playbook is a pre-written document with step-by-step instructions for diagnosing and mitigating a specific known failure

![image-20251113173658607](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131736684.png)

![image-20251113174213427](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131742512.png)



Service level objectives (SLO)

downtime goal, "nines":

- 99% ("2 nines"): 3.65 days / year
- 99.9% ("3 nines"): 8.77 hours / year
- 99.99% ("4 nines"): 52.6 minutes / year
- 99.999% ("5 nines"): 5.26 minutes / year

“5 nines” is an incredibly strict and expensive target. Most services, including Google Search, do not aim for this.



Terms (We focus on SLIs and SLOs. SLAs are for the business and legal teams)

- SLI (Service Level Indicator): The thing you are actually measuring, e.g., The fraction of successful HTTP requests
- SLO (Service Level Objective): The target value for your SLI, e.g., 99.9% of HTTP requests in a month will be successful
- SLA (Service Level Agreement): A business contract, often with financial penalties, e.g., If we fail to meet the 99.9% SLO, we will refund the customer 10% of their bill

An SLO gives you an error budget

- If availability SLO is 99.9%, error budget is 0.1%

  - 0.1% of requests may fail. 1,000,000 requests per month = 1,000 failures allowed

- The error budget is a data-driven tool for making engineering decisions.

  - Budget remaining? It’s safe to launch a risky new feature.

  - Budget exhausted? All hands on deck for reliability work. No new features until the service is stable.



SLOs are a budget, not a goal to be beaten

- If your SLO is 99.9%, and you achieve 99.999% availability, it is not necessarily better

- You're not taking enough risks or launching features quickly enough

![image-20251113175334867](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202511131753964.png)



Outage reason:

- New feature releases (main reason)
- Changing configuration
- Growth of the user base
- A dependency team releasing a new version



# LEC14 Documentation

A new project includes

- Product requirements docs (PRD): for PMs, avoid implementation details, define the target users
- Technical design docs (DDs): for engineers
- API docs: for developers
- Contributor docs (README)
- The code itself



Key sections of a PRD

- Vision / Motivation

- Goals / Non-Goals

- Requirements: ... shall ...
- Success Criteria: The server shall respond to 95% of GET requests in under 200ms



Key sections of a DD

- Objective: technical goals
- Background: context, what system it will touch
- Detailed Design: APIs, Data models, Architecture diagrams
- Alternatives Considered
- Security and Privacy
- Testing Plan
- Performance and Scalability
- Monitoring
- Rollout Plan



API docs

- What does the method do: A clear, one-sentence summary
- What are the parameters and returns
- What exceptions can it throw or what error codes can it return
- Are there any non-obvious pre-conditions or post-conditions?
- A simple example of how to call it

![image-20251206230528930](../Library/Application Support/typora-user-images/image-20251206230528930.png)



make code self-documenting

![image-20251206230637091](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512062306174.png)



Justifying the engineering decisions (trade-offs)

- Economic
  - Development Cost: engineering time
  - Operational Cost: expensive cloud resources (e.g., large databases, powerful VMs)
  - Value vs. Cost trade-off

- Usability
  - User Experience (UX): Is the feature intuitive and easy to understand?
  - Accessibility (a11y): Can people with disabilities use your feature? Does it work with screen readers? Can it be navigated with a keyboard?
  - Discoverability: How will users find this new feature?
- Scalability
  - Load: the system perform under heavy traffic
  - Data Volume: What happens when the amount of data grows by 100x? Does your design for the "List" operation in the CRUD API still work if there are a million entities? (Hint: this is why pagination is important)
  - Concurrency: multiple users try to modify the same resource at the same time
- Maintainability: Can "future you" fix it?
  - Complexity
  - Testability
  - Dependencies: rely on stable, well-supported libraries
- Health and safety
  - Security: potential attacks (path traversal or improper input validation)
  - Abuse: a malicious user exploit this feature to harm other users or the system (Could your file upload feature be used to fill up the server's disk?)
  - Failure Modes: What happens when a dependency (like a database) fails? Does your feature fail gracefully, or does it take down the entire server?

- Legal and political
  - Data Privacy: What user data are you collecting, storing, or displaying? Are you complying with laws like GDPR (in Europe) or CCPA (in California)?
  - Copyright and Licensing: Are you using third-party libraries? Are you complying with their licenses (for example, MIT, GPL)? If your feature handles user-generated content, how do you handle copyright infringement?
  - Terms of Service: Does your feature operate within the bounds of your product's terms of service?

- Ethical
  - Bias: Could your feature perform differently for different groups of people? If you are building a photo-tagging feature, does it work equally well for people of all skin tones?
  - Unintended Consequences: Could your feature be used to enable harassment, spread misinformation, or create filter bubbles?
  - Transparency: If you are recommending content, can the user understand why they are seeing those recommendations?

- Social and cultural
  - Globalization: Does your feature work for users in different countries and cultures? Does it handle different languages, character sets, and date formats?
  - Social Norms: For example, a feature that automatically shares a user's location might be great for a mapping app, but it would be a huge privacy violation in a different context.
  - Inclusivity: Does your feature use language and imagery that is welcoming to all users?

- Environmental
  - Efficiency: Is your feature computationally expensive? A small inefficiency, when run at the scale of millions of users, can have a significant energy cost.
  - Resource Consumption: Does your feature encourage behavior that uses a lot of power (for example, streaming high-resolution video, intensive background processing on a mobile device)?
  - Sustainable Software Engineering: A growing field focused on minimizing the environmental impact of software.



Postmortem: Outage report (never names the engineer who wrote the bug):

- Title: What broke and when.
- Summary: A brief, high-level overview for executives.
- Impact: Who was affected and for how long.
- Root Cause(s): The detailed analysis of the technical and process failures.
- Timeline: A minute-by-minute log of the incident, from detection to resolution.
- What Went Well / Poorly: An honest assessment of the incident response itself.
- Action Items: A list of concrete, assigned tasks to prevent recurrence.

Rretrospective: a small, informal postmortem for a project or a sprint

<img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081544888.png" alt="image-20251208154409787" style="zoom:15%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081544626.png" alt="image-20251208154424578" style="zoom:15%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081544177.png" alt="image-20251208154438130" style="zoom:15%;" />

<img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081544441.png" alt="image-20251208154449401" style="zoom:15%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081545419.png" alt="image-20251208154501382" style="zoom:15%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081545907.png" alt="image-20251208154515865" style="zoom:15%;" />

<img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081545349.png" alt="image-20251208154525313" style="zoom:15%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081545874.png" alt="image-20251208154535836" style="zoom:15%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081546949.png" alt="image-20251208154607900" style="zoom:15%;" />

<img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081546132.png" alt="image-20251208154621090" style="zoom:15%;" /><img src="https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512081546379.png" alt="image-20251208154633331" style="zoom:15%;" />



# LEC15 Deployment & Experiment

Typical deployment environments:

- Devel (Development environment): The "wild west" for developers
  - Latest version of the code, often deployed automatically from the main branch
  - Usually connects to test or development backends, not production data
- Staging: A preview of production for final testing
  - Meant to be an exact replica of the production environment
  - Used for final QA, integration tests, and manual verification
- Canary: A small slice of real users (e.g., 1–5%)
  - If the canary "dies" (i.e., we see errors, crashes, high latency), stop the rollout immediately
- Prod (Production environment): All user traffic
  - Outages or downtime here are a big deal



Rollout: Updating servers without downtime

1. Stop directing traffic to a small set of servers
2. Restart those servers with the new version
3. Run health checks to ensure they are healthy
4. Restore traffic to the updated servers
5. Repeat until all servers are processed



When doing rollout, consider:

-  What code is running that calls me? Can I handle requests from older clients?
- What code is running that I am calling? Can older servers I call handle my new requests?
- Will everything still work if I have to roll back to an old version in a few days or Traffic is split between the old and new versions for a while?

![image-20251206235211000](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512062352078.png)



Solution: Independent protocal definition

Protos (Protocol buffers):

- Language-neutral, platform-neutral, extensible mechanism for serializing structured data
- Defines how data is structured in one place
- Generated source code can write and read structured data to and from a variety of data streams in numerous languages

![image-20251207010748418](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070107462.png)

Update Proto: must make changes that are both:

- Backwards Compatible: Old code can read data written by new code
  - Safe changes:
    - Adding new fields: Old code will just ignore them
    - Marking fields as deprecated
  - Unsafe (breaking) changes:
    - Deleting a required field
    - Changing the type of a field: Example: int32 to string
    - Changing the unique field number
- Forwards Compatible: New code can read data written by old code
  - Data written by old code may be missing some fields. Protos handle this gracefully:
    - If a new field is missing from an old message, the default value is returned. e.g., 0 for integers, empty for strings
    - New code must be able to handle these default values
  - Do not write code that assumes a new field will always be present.



Decoupling deployment from launch

- Rapid deployment: We want to deploy code even if a feature is incomplete
- Foreign dependencies: We want to deploy code but wait to enable it until dependencies are ready
- Risk management: We want to be able to turn off a feature instantly if it is causing problems

Solution: Feature flags

- Present in code as named boolean flags. Example: new-save-button
- A configuration file defines when specific flags should be enabled
- Flags can be enabled for:
  - A percentage of users
  - A specific country or user group
  - Only for company employees

![image-20251207011415595](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070114695.png)

Rapid deployment (Deploying incomplete features):

- Wrap a new, unfinished feature in a flag that is turned off in production
- Allows merging and deploying code incrementally without affecting any users
- The feature is disabled or hidden in production
- Enable for developers in the Devel environment for testing

Foreign dependencies (What happens when a new frontend feature depends on a new backend API):

- Do not try to time your deployments perfectly; this will fail
- Coordinate with flags:
  - Backend team deploys the feature wrapped in a flag that is turned off
  - Frontend team deploys the feature wrapped in a flag that is turned off
  - Once both deployments are complete, teams coordinate to turn both flags on. Even better, use the same flag

Risk management (What happens if you launch a new feature and monitoring dashboards show high error rates and latency)

- The slow way: Start a rollback of the entire deployment
  - This could take an hour or more, during which users are still affected
- The fast way: Go to the feature flag dashboard and turn the flag OFF
  - Within seconds, all users are routed back to the old, stable code path
  - This is the emergency "kill switch"



Rolling the flags

- If flags are defined statically, you would need to re-deploy your server to update values.

- Instead of static configs, make them dynamic!
  - A server can periodically fetch the latest flag configuration from a central place
  - Pushing a new config then affects feature behavior in seconds

- This allows separation of feature flags configs from running binaries

![image-20251207012256260](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070122306.png)



Experiment

Evaluate features: Run an experiment

- Roll it out to a small, random percentage of users
- Compare their behavior to the behavior of users who didn't get the feature

Avoid biased proactive feedback, use A/B testing

![image-20251207012558333](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070125382.png)

- Define key metrics:
  - Goal Metrics: What are you trying to improve?
    - e.g. Click-through rate, conversion rate, session duration, revenue per user
  - Guardrail Metrics: What do you not want to make worse?
    - e.g. Page load time, error rate, uninstalls, support tickets
- Divert your users:
  - Randomly assign users to either the control or treatment group
  - The assignment must be consistent (or "sticky")
  - Users should see the same control or treatment day-over-day (Behavior change takes time)
  - This is typically handled using some sort of stable ID (User ID, Session ID, Cookie)

![image-20251207012859449](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070128499.png)

- Diversion criteria
  - User-level:
    - The most common
    - Ensures a consistent experience for each person
  - Session-level:
    - Each session is randomly assigned
  - Request-level:
    - Each API call is randomly assigned
    - Useful for stateless backend changes (e.g., testing a new search algorithm)
  - Country-level:
    - All users in a country get the same experience. Useful for region-specific launches.



Advanced analysis: Slices & counterfactuals

- Slices: Working with small audiences
  - Imagine you're changing the "Create Playlist" button on YouTube. Only a tiny fraction of users create playlists on any given day. If you run a 1% vs. 1% experiment, the vast majority of users in both groups will not be affected
  - A slice is a targeted narrow population of users affected by a change. Example: users who visited the "Create Playlist" page before.


![image-20251207013249506](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070132555.png)

- Counterfactuals: In an A/B experiment, the control group slice serves as the counterfactual
  - Analysis Example:
    - Average playlists per user in Treatment Slice: 1.2
    - Average playlists per user in Control Slice: 1.0
  - Conclusion: Treatment caused a 20% increase in playlist creation



Interpreting Experiment Results

- If a result is not statistically significant, you cannot claim your feature had an impact
  - Statistical significance is the probability that the observed difference is due to random luck, a.k.a. "p-value"
  - Common value is p < 0.05
  - < 5% chance the result is random
  - = 95% chance the result is real

- Experiment results are rarely a clear win. This is a product and business decision, not just a data decision.
  - Example: Making the "Subscribe" button bigger
  - Goal Metric: Subscriptions increase by 5%
  - Guardrail Metric: Page load time increases by 200ms
  - Guardrail Metric: Revenue from ads on the page decreases by 2%



Decisions after experiments

- Launch:
  - The results are a clear, positive win
  - Roll the feature out to 100% of users
- Iterate:
  - The results are mixed or promising, but not a clear win
  - Go back to the drawing board, make changes, and run a new experiment
- Kill:
  - The results are flat or negative, the feature did not work
  - Archive the code and move on to the next idea
  - This is a hard but critical part of the process!

![image-20251207014434689](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070144735.png)



# LEC16 Team & Career

Core Team:

- Engineers (Individual Contributors / ICs): Responsible for implementation, testing, design, and maintenance

![image-20251207005329040](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070054183.png)

- Engineering Manager (EM):
  - Goal: Ensure the team is productive and happy
  - Focus: Hiring, promotions, career growth, shielding the team from chaos
  - Sometimes technical, often hands-off on code

![image-20251207004839500](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070048592.png)

- Tech lead (TL): In charge of the technical direction of the project
  - Pro: You get to make the big architectural decisions
  - Con: You do the grungy work (fixing builds, monitoring, filling gaps)

![image-20251207004922659](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070049697.png)

Extended Team:

- Product managers (PMs), Aka Program Managers, Producers (in gaming)
  - Understand the market and user needs
  - Define the "What" and the "Why"
  - Prioritize features (deciding what not to build)

![image-20251207005007378](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070050418.png)

![image-20251207005117251](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070051294.png)

![image-20251207005200623](https://cdn.jsdelivr.net/gh/yuhengtu/typora_images@master/img/202512070052668.png)

- Testers & QA
  - Manual Testers:
    - Essential for UI/interactive apps
    - They find the edge cases you didn't think of
  - Software Engineers in Test (SET / SDET):
    - Write code to test code
    - Build automation frameworks and infrastructure
    - Goal: High quality, automated verification

- UI/UX designers & researchers
  - UI Designers:
    - Create the visual look and feel (Photoshop, Figma)
    - Define specs: hex colors, pixel margins, font sizes
  - UX Researchers:
    - The “scientists” of user behavior
    - Conduct usability studies (often behind a one-way mirror)
    - Goal: Validate that the design actually solves the user’s problem

Specialized Role:

- Program Manager (PgM):
  - Focus: Schedules, coordination, dependencies, “Are we on track?”
- Architect / UberTL (tech lead of tech leads):
  - Defines high-level technical vision across teams
  - Ensures systems fit together (for example, “We are all using gRPC”)
- Operations / SRE (Site Reliability Engineering):
  - Goal: Keep the service running
  - Philosophy: Automate operational overhead
  - They carry the pager (means on call for emergency) so you do not have to (hopefully)

- Technical Writers:
  - Write external documentation, API guides, and manuals
  - They translate “engineer-speak” into human language
- Release Engineers:
  - Manage the build and deployment pipeline
  - Ensure releases are reproducible, signed, and safe
  - The Gatekeepers: They control the “Push to Prod” button

Business Side:

- Executives:
  - Set the strategic direction and budget

- Marketing (Product Marketing Managers):
  - Figure out how to position and sell the product
  - They might sell features that don’t exist (yet)
  - They bring in the revenue for your salary

- HR (Human Resources):
  - Role: Hiring, benefits, and handling serious personnel issues
  - When to escalate:
    - When the issue is about people, not code
    - Harassment, discrimination, or creating a hostile work environment
  - Advice:
    - Keep a record (paper trail) of bad behavior
    - HR is there to protect the company, but that often aligns with removing toxic behavior



Career levels (Google level notation)

- Junior (L3/E3):
  - Fixing bugs, small features
  - Needs clear direction: "Implement function X"
- Mid-Level (L4/E4):
  - Owns features or modules
  - Can take a vague task and figure out the details
- Senior (L5/E5):
  - Designs systems, writes Design Docs
  - Asks “What services do we need?”
- Staff/Principal (L6+):
  - Sets technical strategy, solves cross-team problems
  - Starts new projects from scratch



Career is decades, value consistency over intensity, avoid burnout

- Work/life balance goals
  - Rule 1: No all-nighters: Code written at 3 AM is usually garbage anyway
  - Rule 2: Don’t work on weekends: Protect your recharge time
  - Rule 3: Unplug on vacation: If you are checking Slack on the beach, you aren’t on vacation; you are just working remotely with a better view
- A practical setup
  - Separation of Concerns:
    - No work email or Slack on your personal phone. (Or turn off notifications)
    - Use a separate work computer if possible
  - Respecting Others:
    - Scheduled Send: If you work late, schedule emails or messages to arrive at 9:00 AM the next day
    - Don’t normalize after-hours pings

- How to really take a vacation
  - Preparation:
    - Plan early and broadcast your dates
    - Designate a Point of Contact (POC): “If this breaks, talk to Bob.”
  - Execution:
    - Do not respond. If you respond to one email, people will send you five more
  - For the Team:
    - Do not email the person on vacation
    - If they are the only person who knows how to fix something, you need to fix that



Negotiation

- Recommended Reading: Getting More by Stuart Diamond
- The Mindset:
  - Negotiation isn’t about “winning” or “beating” the other person
  - It’s about meeting your goals
    - “Are my actions right now helping me meet my goals?”
    - If getting angry doesn’t help your goal, don’t get angry

- The most important person is them:
  - You can't persuade someone if you don't understand what they value
  - Use empathy. “We both want this project to succeed”
- Be incremental:
  - Don’t ask for the moon immediately
  - Build trust with small wins
- Trade things of unequal value:
  - Trade something cheap for you but valuable to them
  - Example: “I’ll take the on-call shift this weekend if I can take next Friday off (valuable to me).”
- Watch out for these anti-patterns
  - “Brownian Motion”: Everyone is busy, but nobody knows why or what the goal is
  - The Telephone Game: Too many layers of indirection between you and the customer
  - The Zombie Project: No sponsorship from leadership. The project isn’t aligned with business strategy
  - Advice: If you see these signs, try to fix them. If you can’t fix them, it might be time to look for a new team
